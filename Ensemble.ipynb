{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gdown\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, mean_squared_error, roc_curve, roc_auc_score, average_precision_score, ConfusionMatrixDisplay, auc\n",
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe 1 -  Treino: 127549 Validação: 63774 Teste: 63775\n",
      "Classe 2 -  Treino: 67049 Validação: 33524 Teste: 33525\n",
      "Classe 1 -  Treino: 127549 Validação: 63774 Teste: 63775\n",
      "Classe 2 -  Treino: 127549 Validação: 63774 Teste: 33525\n"
     ]
    }
   ],
   "source": [
    "name_file = \"TRNcod.csv\"\n",
    "dataset = pd.read_csv(name_file)\n",
    "dataset.drop(columns=['INDEX'], inplace=True)\n",
    "class_1 = dataset[dataset['IND_BOM_1_1']==1]\n",
    "class_2 = dataset[dataset['IND_BOM_1_2']==1]\n",
    "class_1 = class_1.sample(frac=1)\n",
    "class_2 = class_2.sample(frac=1)\n",
    "class_1_train, class_1_validate, class_1_test = np.split(class_1,[int(0.5*len(class_1)),int(0.75*len(class_1))])\n",
    "class_2_train, class_2_validate, class_2_test = np.split(class_2,[int(0.5*len(class_2)),int(0.75*len(class_2))])  \n",
    "print(\"Classe 1 - \",\"Treino:\",len(class_1_train),\"Validação:\",len(class_1_validate),\"Teste:\",len(class_1_test))\n",
    "print(\"Classe 2 - \",\"Treino:\",len(class_2_train),\"Validação:\",len(class_2_validate),\"Teste:\",len(class_2_test))\n",
    "while len(class_2_train) < len(class_1_train):\n",
    "  count = (len(class_1_train)-len(class_2_train)) % len(class_2_train)\n",
    "  class_2_train = pd.concat([class_2_train[:count],class_2_train])\n",
    "while len(class_2_validate) < len(class_1_validate):\n",
    "  count = (len(class_1_validate)-len(class_2_validate)) % len(class_2_validate)\n",
    "  class_2_validate = pd.concat([class_2_validate[:count],class_2_validate])\n",
    "print(\"Classe 1 - \",\"Treino:\",len(class_1_train),\"Validação:\",len(class_1_validate),\"Teste:\",len(class_1_test))\n",
    "print(\"Classe 2 - \",\"Treino:\",len(class_2_train),\"Validação:\",len(class_2_validate),\"Teste:\",len(class_2_test))\n",
    "data_train = pd.concat([class_1_train,class_2_train])\n",
    "data_train = data_train.sample(frac=1)\n",
    "data_validate = pd.concat([class_1_validate,class_2_validate])\n",
    "data_validate = data_validate.sample(frac=1)\n",
    "data_test = pd.concat([class_1_test,class_2_test])\n",
    "data_test = data_test.sample(frac=1)\n",
    "x_train = data_train.iloc[:,:-2]\n",
    "y_train = data_train.iloc[:,-2:]\n",
    "x_validate = data_validate.iloc[:,:-2]\n",
    "y_validate = data_validate.iloc[:,-2:]\n",
    "x_test = data_test.iloc[:,:-2]\n",
    "y_test = data_test.iloc[:,-2:]\n",
    "y_train2 = y_train.drop(columns=['IND_BOM_1_2'])\n",
    "y_validate2 = y_validate.drop(columns=['IND_BOM_1_2'])\n",
    "y_test2 = y_test.drop(columns=['IND_BOM_1_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_metrics(y_true,y_pred):\n",
    "  y_true = y_true['IND_BOM_1_1']\n",
    "  y_pred = y_pred['IND_BOM_1_1']\n",
    "  ConfusionMatrixDisplay.from_predictions(y_true, y_pred, normalize='true')\n",
    "  print(classification_report(y_true,y_pred))\n",
    "  print(\"Erro quadrático médio: \",mean_squared_error(y_true,y_pred))\n",
    "  print(\"Auroc:\", roc_auc_score(y_true,y_pred))\n",
    "  print(\"Precisão média: \", average_precision_score(y_true, y_pred))\n",
    "  print()\n",
    "\n",
    "@tf.function\n",
    "def ks(y_true,y_pred):\n",
    "    y_true = tf.reshape(y_true,(-1,))\n",
    "    y_pred = tf.reshape(y_pred,(-1,))\n",
    "    length = tf.shape(y_true)[0]\n",
    "    t = tf.math.top_k(y_pred,k = length,sorted =False)\n",
    "    y_pred_sorted = tf.gather(y_pred,t.indices)\n",
    "    y_true_sorted = tf.gather(y_true,t.indices)\n",
    "    cum_positive_ratio = tf.truediv(\n",
    "        tf.cumsum(y_true_sorted),tf.reduce_sum(y_true_sorted))\n",
    "    cum_negative_ratio = tf.truediv(\n",
    "        tf.cumsum(1- y_true_sorted),tf.reduce_sum(1- y_true_sorted))\n",
    "    ks_value = tf.reduce_max(tf.abs(cum_positive_ratio - cum_negative_ratio))\n",
    "    return ks_value\n",
    "\n",
    "def pred_name_columns(arr):\n",
    "    result = pd.DataFrame(arr, columns = ['IND_BOM_1_1'])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import layers, optimizers, losses\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IND_BOM_1_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>255098.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         IND_BOM_1_1\n",
       "count  255098.000000\n",
       "mean        0.500000\n",
       "std         0.500001\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.500000\n",
       "75%         1.000000\n",
       "max         1.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        IND_BOM_1_1  IND_BOM_1_2\n",
      "10836             1            0\n",
      "139288            1            0\n",
      "34428             1            0\n",
      "310857            1            0\n",
      "230865            1            0\n",
      "...             ...          ...\n",
      "180880            1            0\n",
      "88597             1            0\n",
      "249615            0            1\n",
      "230970            1            0\n",
      "289306            1            0\n",
      "\n",
      "[255098 rows x 2 columns]\n",
      "        IND_BOM_1_1\n",
      "10836             1\n",
      "139288            1\n",
      "34428             1\n",
      "310857            1\n",
      "230865            1\n",
      "...             ...\n",
      "180880            1\n",
      "88597             1\n",
      "249615            0\n",
      "230970            1\n",
      "289306            1\n",
      "\n",
      "[255098 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)\n",
    "print(y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6688/2463731992.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m                     early_stopping=True, n_iter_no_change=20, validation_fraction=0.1)\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mensemble_classifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStackingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrf_classifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlp8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mensemble_pred_class_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensemble_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_validate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\ensemble\\_stacking.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    486\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_le\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_le\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_le\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"final_estimator_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\ensemble\\_stacking.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[1;31m# all_estimators contains all estimators, the one to be fitted and the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[1;31m# 'drop' string.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_estimators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_estimators\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_final_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\ensemble\\_base.py\u001b[0m in \u001b[0;36m_validate_estimators\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    245\u001b[0m                 \u001b[1;34m\" of (string, estimator) tuples.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m             )\n\u001b[1;32m--> 247\u001b[1;33m         \u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m         \u001b[1;31m# defined by MetaEstimatorMixin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100,max_depth=8,min_samples_leaf=2,min_samples_split=4,max_features='sqrt')\n",
    "mlp8 = MLPClassifier(hidden_layer_sizes=(50,50), solver='adam', activation='relu', learning_rate_init=0.005, max_iter=10000, \n",
    "                    early_stopping=True, n_iter_no_change=20, validation_fraction=0.1)\n",
    "mlp9 = MLPClassifier(hidden_layer_sizes=(100,50,25), solver='adam', activation='relu', learning_rate_init=0.005, max_iter=10000, \n",
    "                    early_stopping=True, n_iter_no_change=20, validation_fraction=0.1)\n",
    "\n",
    "ensemble_classifier = StackingClassifier(estimators=[('dt', rf_classifier, mlp8, mlp9)]).fit(x_train.to_numpy(), y_train2.to_numpy().ravel())\n",
    "\n",
    "ensemble_pred_class_val = ensemble_classifier.predict(x_validate)\n",
    "y_pred_ensemble_val = pred_name_columns(ensemble_pred_class_val)\n",
    "my_metrics(y_validate,y_pred_ensemble_val)\n",
    "\n",
    "ensemble_prb_val = ensemble_classifier.predict_proba(x_validate)\n",
    "skplt.metrics.plot_ks_statistic(y_validate['IND_BOM_1_1'], ensemble_prb_val[0])\n",
    "plt.show()\n",
    "skplt.metrics.plot_ks_statistic(y_validate['IND_BOM_1_2'], ensemble_prb_val[1])\n",
    "plt.show()\n",
    "\n",
    "ensemble_pred_class_test = ensemble_classifier.predict(x_test)\n",
    "y_pred_ensemble_test = pred_name_columns(ensemble_pred_class_test)\n",
    "my_metrics(y_test,y_pred_ensemble_test)\n",
    "\n",
    "ensemble_prb_test = ensemble_classifier.predict_proba(x_test)\n",
    "skplt.metrics.plot_ks_statistic(y_test['IND_BOM_1_1'], ensemble_prb_test[0])\n",
    "plt.show()\n",
    "skplt.metrics.plot_ks_statistic(y_test['IND_BOM_1_2'], ensemble_prb_test[1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e5a903eea30aa08e1f9d27b74a5ed310161c979055b2deac4a4d6b5123a4d6b6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
